#!/usr/bin/env python3
#

# Use threads on same node, MPI between nodes
# No MPI on interactive runs (but threads OK) 

from __future__ import absolute_import
from __future__ import print_function
from __future__ import division

import sys
import os, os.path
import argparse
import tempfile
import string

INSTALL_DIR="/prog/pflotran/build-20240419-gcc"



def create_mpi_job_script_pbs(vardict, f):
    
    print(
        """#!/bin/bash
        
WORKDIR="{workdir}"
CASE="{case}"

case "$( uname -r )" in
    3.*) TOP_DIR="{install_dir}-rh7"
         ;;
    4.*) TOP_DIR="{install_dir}-rh8"
         ;;
    *)
         echo "Unsupported OS" >&2
         ;;
esac
export PETSC_DIR="$TOP_DIR/petsc"
export PETSC_ARCH=arch-linux-c-opt
export LD_LIBRARY_DIR=="$PETSC_DIR/lib"
MPIRUN="$PETSC_DIR/bin/mpirun"
EXE="$TOP_DIR/pflotran_ogs_1.8/src/pflotran/pflotran"

cd {workdir}


# -display-map is just for debugging MPI layout 

# Check for possibly non-working RDMA transport
# -display-map is just for debugging MPI layout 
if /usr/sbin/lsmod | egrep -qw bnxt_re
then
    $MPIRUN -mca btl 'vader,self,tcp' -mca pml ^ucx -display-map -machinefile $MFILE -np $NP $EXE -pflotranin $CASE.in > $CASE.LOG 2> $CASE.ERR
else
    $MPIRUN -display-map -machinefile $MFILE -np $NP $EXE -pflotranin $CASE.in > $CASE.LOG 2> $CASE.ERR
fi

""".format(**vardict), file=f )
    
def create_job_script(vardict, f):
    
    print(
        """#!/bin/bash
        
WORKDIR="{workdir}"
CASE="{case}"

case "$( uname -r )" in
    3.*) TOP_DIR="{install_dir}-rh7"
         ;;
    4.*) TOP_DIR="{install_dir}-rh8"
         ;;
    *)
         echo "Unsupported OS" >&2
         ;;
esac
export PETSC_DIR="$TOP_DIR/petsc"
export PETSC_ARCH=arch-linux-c-opt
# Help make sure no other system lib is preferred
export LD_LIBRARY_DIR="$PETSC_DIR/lib"

MPIRUN="$PETSC_DIR/bin/mpirun"
EXE="$TOP_DIR/pflotran_ogs_1.8/src/pflotran/pflotran"

cd "{workdir}"

NP="{nmpi}"
MPITRANSPORT=""
MFILE=""
DELFILE=""

if [ -n "$LSB_MCPU_HOSTS" ]
then
    # LSF
    #if [ -r "$LSB_DJOB_RANKFILE" ]
    #then
        NP=`wc -l  < $LSB_DJOB_RANKFILE`
        MFILE="-machinefile $LSB_DJOB_RANKFILE"
    #elif [ -r "$LSB_DJOB_HOSTFILE" ]
    #then
    #    NP=`wc -l  < $LSB_DJOB_HOSTFILE`
    #    MFILE="-machinefile $LSB_DJOB_HOSTFILE"
    #else
    #    MFILE="$CASE_mpi.$$"
    #    DELFILE="$CASE_mpi.$$"
    #    MSTR=$( $TOP_DIR/../bin/create_machinefile $MFILE {nmpi_per_node} )
    #    MFILE="-machinefile $MFILE"
    #fi
elif [ -n "$PBS_NODEFILE" ]
then
    #
    # PBS has one thread per line - no need to transform the file
    #
    NP=`wc -l  < $PBS_NODEFILE`
    MFILE="-machinefile $PBS_NODEFILE"
    # cd $PBS_O_WORKDIR
else
    RUNINTERACTIVE="y"
fi

# Check for possibly non-working RDMA transport
if /usr/sbin/lsmod | egrep -qw bnxt_re
then
    MPITRANSPORT="-mca btl vader,self,tcp -mca pml ^ucx"
fi

if [ -n "$RUNINTERACTIVE" ]
then
    # Use stdout/stderr swap trick to pipe to different files as well as both to terminal..
    ($MPIRUN --bind-to none -np $NP $EXE -pflotranin "$CASE.in" | tee "$CASE.LOG") 3>&1 1>&2 2>&3 | tee "$CASE.ERR"
else
    # -display-map is just for debugging MPI layout 
    $MPIRUN -display-map $MPITRANSPORT $MFILE -np $NP $EXE -pflotranin "$CASE.in" > "$CASE.LOG" 2>"$CASE.ERR"
fi

[ -s "$CASE-mas.dat" ] && cat "$CASE-mas.dat" | perl -pe 's/,/;/g' | perl -pe 's/^  //g' | perl -pe 's/  /;/g'  > "$CASE-mas.csv"

[ -n "$DELFILE" ] && rm -f "$DELFILE"

""".format(**vardict), file=f )


def main(argv = sys.argv):
    pbscluster = os.path.exists("/opt/pbs/bin/qsub")
    parser = argparse.ArgumentParser(description="Run PFLOTRAN (OGS version)")
    parser.add_argument("data_file", help="PFLOTRAN data file")
    if pbscluster:
        parser.add_argument("-q", "--queue", default="normal", help="Simulation queue (default=normal)")
    else:
        parser.add_argument("-q", "--queue", default="bigmem8", help="Simulation queue (default=bigmem8)")
    parser.add_argument("-nm", "--num_mpi_per_node", type=int, default=1, help="Number of MPI processes per node")
    parser.add_argument("-nn", "--num_nodes", type=int, default=1, help="Number of nodes")
    parser.add_argument("-i", "--interactive", action="store_true", help="Interactive (local node) run (default=no)")
    parser.add_argument("-e", "--exclusive", action="store_true", help="Exclusive node usage (default=shared). Not allowed in LSF clusters")
    args = parser.parse_args()

    abs_data_file = os.path.abspath(args.data_file)
    workdir, data_file = os.path.split(abs_data_file)
    case_name, ext = os.path.splitext(data_file)
    if ext != ".in":
        print("ERROR: PFLOTRAN deck must have extension .in")
        return 1

    queue = args.queue



    nthreads_per_mpi = 1
    nmpi_per_node = args.num_mpi_per_node
    nnodes = args.num_nodes
                        
    interactive = args.interactive
    if interactive and nnodes > 1:
        print("Cannot specify > 1 nodes for interactive runs. Reverting #nodes to 1")
        nnodes = 1

    if args.exclusive:
        placement="scatter:excl"
    else:
        placement="scatter:shared"

    nmpi = nmpi_per_node*nnodes
    ncpu = nthreads_per_mpi*nmpi
    nthreads_per_node = nmpi_per_node*nthreads_per_mpi
                        
    vardict = {
        'workdir' : workdir,
        'case' : case_name,
        'install_dir' : INSTALL_DIR,
        'nmpi_per_node' : nmpi_per_node,
        'nthreads_per_mpi' : nthreads_per_mpi,        
        'nnodes' : nnodes,
        'ncpu' : ncpu,
        'nmpi' : nmpi,
        }

    jobfilename = "{workdir}/{case}_job.run".format(**vardict)
    with open(jobfilename, "w") as f:
        create_job_script(vardict, f)

    os.system("chmod ug+x {}".format(jobfilename))
    if interactive:
        cmd = "{}".format(jobfilename)
        os.system(cmd)
    else:

        rstring = '-R \"select[x86_64Linux] span[ptile={}] same[type:model]'.format(nthreads_per_node)        
        if nthreads_per_node > 1 and nthreads_per_node < 5:
            rstring += ' affinity[core({})]\"'.format(nthreads_per_node)
        else:
            rstring += '\"'
           
        if pbscluster:
            #cmd = 'qsub -q {q} -l nodes={nodes}:ppn={ppn} -N \"PF_{cn}\" -j oe -o {wd}/{cn}_qsub.LOG {job}'.format(q=queue, nodes=nnodes, ppn=nmpi_per_node, wd=workdir, cn=case_name, job=jobfilename)
            cmd = 'qsub -q {q} -l select={nodes}:ncpus={ppn}:mpiprocs={ppn} -l place={place} -N \"PF_{cn}\" -j oe -o {wd}/{cn}_qsub.LOG {job}'.format(q=queue, nodes=nnodes, ppn=nmpi_per_node, place=placement, wd=workdir, cn=case_name, job=jobfilename)
        else:
            cmd = 'bsub -L /bin/csh -q {q} -n {n} -o {wd}/{cn}_bsub.LOG -J \"RUNPFLOTRAN {cn}\" {rs} {job}'.format(q=queue, n=ncpu, wd=workdir, cn=case_name, rs=rstring, job=jobfilename)


        print("#################################################################################")
        print(cmd)
        print("#################################################################################")
        os.system(cmd)
        
    return 0
        

if __name__ == "__main__":
    sys.exit(main())

